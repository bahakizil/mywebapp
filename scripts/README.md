# Data Scraping Scripts

Bu klasÃ¶r LinkedIn ve Medium'dan gerÃ§ek engagement verilerini Ã§ekmeye yarayan script'leri iÃ§erir.

## ğŸ“‹ Ä°Ã§indekiler

- `linkedin-scraper.py` - LinkedIn postlarÄ±nÄ± gerÃ§ek likes, comments, shares verileriyle Ã§eker
- `medium-scraper.py` - Medium makalelerini gerÃ§ek claps, responses, views verileriyle Ã§eker
- `daily-scraper.py` - Her iki scraper'Ä± koordine eden ana script
- `setup-cron.sh` - GÃ¼nlÃ¼k otomatik Ã§alÄ±ÅŸtÄ±rma kurulumu
- `requirements.txt` - Python kÃ¼tÃ¼phaneleri

## ğŸš€ Kurulum

### 1. Gereksinimler

```bash
# Python 3.8+ ve pip gerekli
python3 --version
pip3 --version

# Chrome browser gerekli (headless mode iÃ§in)
google-chrome --version
```

### 2. KÃ¼tÃ¼phaneleri YÃ¼kle

```bash
cd scripts
pip install -r requirements.txt
```

### 3. Environment Variables

LinkedIn scraper iÃ§in giriÅŸ bilgileri gerekli:

```bash
export LINKEDIN_EMAIL="your-email@example.com"
export LINKEDIN_PASSWORD="your-password"
```

## ğŸ”§ KullanÄ±m

### Manuel Ã‡alÄ±ÅŸtÄ±rma

```bash
# LinkedIn scraper'Ä± Ã§alÄ±ÅŸtÄ±r
cd scripts
LINKEDIN_EMAIL="email" LINKEDIN_PASSWORD="pass" python linkedin-scraper.py

# Medium scraper'Ä± Ã§alÄ±ÅŸtÄ±r
python medium-scraper.py

# Her ikisini birden Ã§alÄ±ÅŸtÄ±r
LINKEDIN_EMAIL="email" LINKEDIN_PASSWORD="pass" python daily-scraper.py
```

### Otomatik GÃ¼nlÃ¼k Ã‡alÄ±ÅŸtÄ±rma

```bash
# Cron job kurulumu (her gÃ¼n saat 08:00)
cd scripts
./setup-cron.sh

# Manuel test
python daily-scraper.py

# Log'larÄ± kontrol et
tail -f ../logs/daily-scraper.log
```

## ğŸ“Š Ã‡Ä±ktÄ± Verileri

### LinkedIn Posts (`data/linkedin-posts.json`)

```json
{
  "lastUpdated": "2024-01-15T08:00:00Z",
  "nextUpdate": "2024-01-16T08:00:00Z",
  "source": "selenium-scraper-enhanced",
  "posts": [
    {
      "id": "real-1705320000-0",
      "text": "I'm happy to share that I've been accepted...",
      "publishedAt": "2024-01-10T10:30:00Z",
      "author": {
        "name": "Baha Kizil",
        "headline": "AI Engineer & Computer Vision Specialist"
      },
      "engagement": {
        "likes": 89,
        "comments": 12,
        "shares": 7
      },
      "url": "https://www.linkedin.com/posts/bahakizil_ai-bootcamp-...",
      "scrapedAt": "2024-01-15T08:00:00Z"
    }
  ]
}
```

### Medium Articles (`data/medium-articles.json`)

```json
{
  "lastUpdated": "2024-01-15T08:05:00Z",
  "nextUpdate": "2024-01-16T08:05:00Z",
  "source": "selenium-scraper-enhanced",
  "articles": [
    {
      "title": "Building Production-Ready AI Applications...",
      "link": "https://medium.com/@bahakizil/building-scalable...",
      "publishedDate": "2024-01-10T14:20:00Z",
      "description": "A comprehensive guide to building...",
      "categories": ["AI", "Next.js", "Production"],
      "author": "Baha Kizil",
      "engagement": {
        "claps": 47,
        "responses": 8,
        "views": 340
      },
      "scrapedAt": "2024-01-15T08:05:00Z"
    }
  ]
}
```

## ğŸ›¡ï¸ GÃ¼venlik

- LinkedIn credentials'larÄ± environment variable'larda saklayÄ±n
- Script'leri private repository'lerde tutun
- Rate limiting aktif (2 saniye bekleme)
- Headless browser kullanÄ±mÄ±
- User-agent rotation

## ğŸ” Troubleshooting

### Chrome Driver HatasÄ±

```bash
# ChromeDriver'Ä± manuel yÃ¼kle
pip install webdriver-manager
```

### LinkedIn GiriÅŸ Problemi

```bash
# 2FA aktifse app password kullanÄ±n
# IP whitelist kontrolÃ¼ yapÄ±n
# ÅÃ¼pheli aktivite uyarÄ±sÄ± varsa bekleyin
```

### Medium Scraping HatasÄ±

```bash
# RSS fallback aktif
# Rate limiting nedeniyle yavaÅŸ Ã§alÄ±ÅŸabilir
# VPN kullanarak deneyin
```

### Cron Job Ã‡alÄ±ÅŸmÄ±yor

```bash
# Cron job'larÄ± kontrol et
crontab -l

# Log'larÄ± kontrol et
tail -f ../logs/daily-scraper.log

# Manuel test
cd scripts && python daily-scraper.py
```

## ğŸ“… GÃ¼ncelleme PlanÄ±

- **GÃ¼nlÃ¼k**: LinkedIn ve Medium verileri otomatik gÃ¼ncellenir
- **HaftalÄ±k**: Scraper performansÄ± log'lardan kontrol edilir
- **AylÄ±k**: Script'ler ve kÃ¼tÃ¼phaneler gÃ¼ncellenir

## ğŸ”§ GeliÅŸtirme

### Yeni Platform Ekleme

1. `new-platform-scraper.py` oluÅŸtur
2. `daily-scraper.py`'a entegre et
3. `requirements.txt`'e kÃ¼tÃ¼phaneleri ekle
4. API interface'ini gÃ¼ncelle

### Performans Optimizasyonu

- Paralel scraping implementasyonu
- Caching mekanizmasÄ±
- Incremental updates
- Error recovery

## ğŸ“ Destek

Sorun yaÅŸarsanÄ±z:
1. Log dosyalarÄ±nÄ± kontrol edin
2. Manual test Ã§alÄ±ÅŸtÄ±rÄ±n
3. GitHub Issues aÃ§Ä±n
4. Documentation gÃ¼ncelleyin

---

**Not**: Bu script'ler web scraping kullanÄ±r. Platform'larÄ±n terms of service'lerini kontrol edin ve respectful usage yapÄ±n. 