# Portfolio Setup Guide - Static Data System

## ðŸ“‹ Overview

Bu portfÃ¶lio sitesi artÄ±k **static data cache sistemi** kullanÄ±yor. Data, gÃ¼nde 1 kere otomatik olarak Ã§ekiliyor ve dosyalara kaydediliyor. Bu sayede:

- âœ… **HÄ±zlÄ± sayfa yÃ¼kleme** - Her ziyaretÃ§i iÃ§in API Ã§aÄŸrÄ±sÄ± yok
- âœ… **API rate limiting yok** - GitHub/Medium API'lerinden korunuyor
- âœ… **GÃ¼venilir performans** - API down olsa bile site Ã§alÄ±ÅŸÄ±yor
- âœ… **GÃ¼ncel data** - GÃ¼nde 1 kere taze data

## ðŸ”§ Sistem NasÄ±l Ã‡alÄ±ÅŸÄ±yor?

### 1. Data Sync Process
```bash
# Manuel sync
npm run sync-data

# Otomatik gÃ¼nlÃ¼k sync (cron job)
./scripts/daily-sync.sh
```

### 2. Data DosyalarÄ±
- `data/portfolio-data.json` - TÃ¼m data burada saklanÄ±yor
- `logs/sync-*.log` - Sync loglarÄ± (7 gÃ¼n saklanÄ±yor)

### 3. API Endpoints
- `/api/repos` - GitHub repositories (static)
- `/api/medium` - Medium articles (static)  
- `/api/linkedin` - LinkedIn posts (static)

## ðŸš€ Production Setup

### 1. Environment Variables
`.env.local` dosyasÄ±na ekle:
```env
GITHUB_USERNAME=bahakizil
GITHUB_TOKEN=your_github_token
MEDIUM_USERNAME=@bahakizil
```

### 2. Ä°lk Data Sync
```bash
npm run sync-data
```

### 3. Cron Job Setup (GÃ¼nlÃ¼k Otomatik Sync)

#### macOS/Linux:
```bash
# Crontab'Ä± aÃ§
crontab -e

# Her gÃ¼n saat 02:00'da Ã§alÄ±ÅŸacak ÅŸekilde ekle
0 2 * * * /Users/bahakizil/Downloads/mypage/scripts/daily-sync.sh

# Crontab'Ä± kaydet ve Ã§Ä±k
```

#### Windows (Task Scheduler):
1. Task Scheduler'Ä± aÃ§
2. "Create Basic Task" tÄ±kla
3. Daily, 02:00 olarak ayarla
4. Script: `scripts/daily-sync.sh`

### 4. Build Process
Build sÄ±rasÄ±nda otomatik sync:
```bash
npm run build  # Otomatik olarak npm run sync-data Ã§alÄ±ÅŸÄ±r
```

## ðŸ“Š Data KaynaklarÄ±

### GitHub Repositories
- **Kaynak**: GitHub API
- **Limit**: Authenticated: 5000/saat, Unauthenticated: 60/saat
- **Cache**: 24 saat
- **Fallback**: Mock data

### Medium Articles  
- **Kaynak**: RSS Feed
- **Limit**: Yok
- **Cache**: 24 saat
- **Fallback**: Mock data

### LinkedIn Posts
- **Kaynak**: Scraped JSON file veya mock data
- **Cache**: 24 saat
- **Real Data**: `scripts/linkedin-scraper.py` ile

## ðŸ” Troubleshooting

### Data GÃ¼ncellenmiyorsa:
```bash
# Manuel sync test
npm run sync-data

# Log kontrolÃ¼
tail -f logs/sync-$(date +%Y%m%d).log

# Cache temizle
rm -f data/portfolio-data.json
npm run sync-data
```

### API HatalarÄ±nda:
- GitHub: Rate limit kontrolÃ¼
- Medium: RSS feed eriÅŸimi kontrolÃ¼
- LinkedIn: Mock data kullanÄ±lÄ±yor

### Cron Job Ã‡alÄ±ÅŸmÄ±yorsa:
```bash
# Cron job listesi
crontab -l

# Cron service durumu (Linux)
systemctl status cron

# Log kontrolÃ¼
grep CRON /var/log/syslog
```

## ðŸ“ˆ Monitoring

### Sync Status
```bash
# Son sync zamanÄ±
cat data/portfolio-data.json | grep lastUpdated

# Log dosyalarÄ±
ls -la logs/
```

### Performance
- **Ä°lk yÃ¼kleme**: ~2-3 saniye
- **Sonraki yÃ¼klemeler**: ~200-500ms (cache sayesinde)
- **API response**: ~50-100ms (static files)

## ðŸ”„ Manual Update

Acil veri gÃ¼ncellemesi gerekirse:
```bash
# Data sync
npm run sync-data

# Development server restart (gerekirse)
npm run dev
```

## ðŸ“ Maintenance

### HaftalÄ±k:
- Log dosyalarÄ±nÄ± kontrol et
- Sync baÅŸarÄ± oranÄ±nÄ± kontrol et

### AylÄ±k:  
- GitHub token'Ä± kontrol et
- Medium RSS feed'i test et
- LinkedIn scraper'Ä± gÃ¼ncelle

### API Token Yenileme:
1. GitHub: Settings > Developer settings > Personal access tokens
2. Medium: RSS feed, token gerektirmiyor
3. LinkedIn: Manuel scraping, token yok

## ðŸŽ¯ Next Steps

1. **Real LinkedIn Data**: `scripts/linkedin-scraper.py` kullan
2. **CDN Setup**: Cloudflare ile cache optimization
3. **Webhook Integration**: Auto-deploy on data changes
4. **Analytics**: Data sync success tracking 